#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å ±å‘Šç”Ÿæˆæ¨¡çµ„
============

ç”Ÿæˆ AIO æ½›åŠ›åˆ†æçš„ç¶œåˆå ±å‘Šï¼Œ
æ”¯æŒå¤šç¨®æ ¼å¼è¼¸å‡ºå’Œåœ–è¡¨ç”Ÿæˆã€‚
"""

import os
import pandas as pd
from datetime import datetime
from pathlib import Path
from typing import Dict, Any, Optional, List
import json

from .logger import LoggingMixin


class ReportGenerator(LoggingMixin):
    """å ±å‘Šç”Ÿæˆå™¨"""
    
    def __init__(self, config: Dict[str, Any], logger=None):
        """
        åˆå§‹åŒ–å ±å‘Šç”Ÿæˆå™¨
        
        Args:
            config: é…ç½®å­—å…¸
            logger: æ—¥èªŒè¨˜éŒ„å™¨
        """
        self.config = config
        if logger:
            self._logger = logger
        
        # è¼¸å‡ºé…ç½®
        self.output_config = config.get('output', {})
        self.output_dir = Path(self.output_config.get('directory', 'output'))
        self.output_format = self.output_config.get('format', 'csv')
        self.include_charts = self.output_config.get('include_charts', True)
        self.timestamp_format = self.output_config.get('timestamp_format', '%Y%m%d_%H%M%S')
        
        # ç¢ºä¿è¼¸å‡ºç›®éŒ„å­˜åœ¨
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        self.logger.info(f"å ±å‘Šç”Ÿæˆå™¨å·²åˆå§‹åŒ– - è¼¸å‡ºç›®éŒ„: {self.output_dir}")
    
    def _generate_filename(self, base_name: str, extension: str = None) -> str:
        """
        ç”Ÿæˆå¸¶æ™‚é–“æˆ³çš„æ–‡ä»¶å
        
        Args:
            base_name: åŸºç¤æ–‡ä»¶å
            extension: æ–‡ä»¶æ“´å±•å
            
        Returns:
            å®Œæ•´çš„æ–‡ä»¶å
        """
        timestamp = datetime.now().strftime(self.timestamp_format)
        
        if extension:
            return f"{base_name}_{timestamp}.{extension}"
        else:
            return f"{base_name}_{timestamp}"
    
    async def generate_detailed_report(self,
                                     analysis_data: pd.DataFrame,
                                     gsc_data: pd.DataFrame,
                                     output_dir: str = None) -> str:
        """
        ç”Ÿæˆè©³ç´°çš„ AIO åˆ†æå ±å‘Š
        
        Args:
            analysis_data: åˆ†æçµæœæ•¸æ“š
            gsc_data: GSC åŸå§‹æ•¸æ“š
            output_dir: è¼¸å‡ºç›®éŒ„ï¼ˆå¯é¸ï¼‰
            
        Returns:
            ä¸»å ±å‘Šæ–‡ä»¶è·¯å¾‘
        """
        if output_dir:
            self.output_dir = Path(output_dir)
            self.output_dir.mkdir(parents=True, exist_ok=True)
        
        self.logger.info("é–‹å§‹ç”Ÿæˆè©³ç´°å ±å‘Š...")
        
        # æ•¸æ“šé è™•ç†
        processed_data = self._preprocess_data(analysis_data, gsc_data)
        
        # ç”Ÿæˆä¸»å ±å‘Š
        main_report_path = await self._generate_main_report(processed_data)
        
        # ç”Ÿæˆæ‘˜è¦å ±å‘Š
        summary_path = await self._generate_summary_report(processed_data)
        
        # ç”Ÿæˆçµ±è¨ˆå ±å‘Š
        stats_path = await self._generate_statistics_report(processed_data)
        
        # å¦‚æœå•Ÿç”¨ï¼Œç”Ÿæˆåœ–è¡¨
        if self.include_charts:
            await self._generate_charts(processed_data)
        
        # ç”Ÿæˆå…ƒæ•¸æ“šæ–‡ä»¶
        await self._generate_metadata(processed_data, main_report_path)
        
        self.logger.info(f"å ±å‘Šç”Ÿæˆå®Œæˆ - ä¸»å ±å‘Š: {main_report_path}")
        
        return str(main_report_path)
    
    def _preprocess_data(self, analysis_data: pd.DataFrame, gsc_data: pd.DataFrame) -> Dict[str, Any]:
        """
        é è™•ç†æ•¸æ“šä»¥ä¾›å ±å‘Šç”Ÿæˆ
        
        Args:
            analysis_data: åˆ†æçµæœæ•¸æ“š
            gsc_data: GSC åŸå§‹æ•¸æ“š
            
        Returns:
            è™•ç†å¾Œçš„æ•¸æ“šå­—å…¸
        """
        processed = {
            'analysis_data': analysis_data.copy() if not analysis_data.empty else pd.DataFrame(),
            'gsc_data': gsc_data.copy() if not gsc_data.empty else pd.DataFrame(),
            'timestamp': datetime.now(),
            'summary_stats': {}
        }
        
        # è¨ˆç®—æ‘˜è¦çµ±è¨ˆ
        if not analysis_data.empty:
            total_keywords = len(analysis_data)
            aio_keywords = analysis_data['triggers_aio'].sum() if 'triggers_aio' in analysis_data.columns else 0
            aio_percentage = (aio_keywords / total_keywords * 100) if total_keywords > 0 else 0
            
            # æœå°‹é‡çµ±è¨ˆ
            search_volume_stats = {}
            if 'search_volume' in analysis_data.columns:
                search_volume_stats = {
                    'total_volume': analysis_data['search_volume'].sum(),
                    'avg_volume': analysis_data['search_volume'].mean(),
                    'median_volume': analysis_data['search_volume'].median(),
                    'max_volume': analysis_data['search_volume'].max()
                }
            
            # ç«¶çˆ­ç¨‹åº¦åˆ†å¸ƒ
            competition_distribution = {}
            if 'competition_level' in analysis_data.columns:
                competition_distribution = analysis_data['competition_level'].value_counts().to_dict()
            
            processed['summary_stats'] = {
                'total_keywords': total_keywords,
                'aio_keywords': aio_keywords,
                'aio_percentage': round(aio_percentage, 2),
                'search_volume_stats': search_volume_stats,
                'competition_distribution': competition_distribution
            }
        
        # GSC æ•¸æ“šçµ±è¨ˆ
        if not gsc_data.empty:
            gsc_stats = {
                'total_queries': len(gsc_data),
                'total_clicks': gsc_data['clicks'].sum() if 'clicks' in gsc_data.columns else 0,
                'total_impressions': gsc_data['impressions'].sum() if 'impressions' in gsc_data.columns else 0,
                'avg_ctr': gsc_data['ctr'].mean() if 'ctr' in gsc_data.columns else 0,
                'avg_position': gsc_data['position'].mean() if 'position' in gsc_data.columns else 0
            }
            processed['gsc_stats'] = gsc_stats
        
        return processed
    
    async def _generate_main_report(self, data: Dict[str, Any]) -> Path:
        """ç”Ÿæˆä¸»è¦åˆ†æå ±å‘Š"""
        filename = self._generate_filename('aio_analysis_report', 'csv')
        file_path = self.output_dir / filename
        
        analysis_df = data['analysis_data']
        
        if not analysis_df.empty:
            # é‡æ–°å‘½åæ¬„ä½ç‚ºä¸­æ–‡
            column_mapping = {
                'keyword_idea': 'ç›®æ¨™é—œéµå­—',
                'search_volume': 'æ¯æœˆæœå°‹é‡',
                'competition_level': 'ç«¶çˆ­ç¨‹åº¦',
                'triggers_aio': 'è§¸ç™¼AIO',
                'competition_index': 'ç«¶çˆ­æŒ‡æ•¸',
                'low_top_of_page_bid_usd': 'æœ€ä½å‡ºåƒ¹(USD)',
                'high_top_of_page_bid_usd': 'æœ€é«˜å‡ºåƒ¹(USD)'
            }
            
            report_df = analysis_df.rename(columns=column_mapping)
            
            # è½‰æ›å¸ƒæ—å€¼ç‚º Y/N
            if 'è§¸ç™¼AIO' in report_df.columns:
                report_df['è§¸ç™¼AIO'] = report_df['è§¸ç™¼AIO'].apply(lambda x: 'Y' if x else 'N')
            
            # æ’åºï¼šå„ªå…ˆé¡¯ç¤ºè§¸ç™¼ AIO çš„é—œéµå­—ï¼Œç„¶å¾ŒæŒ‰æœå°‹é‡æ’åº
            if 'æ¯æœˆæœå°‹é‡' in report_df.columns and 'è§¸ç™¼AIO' in report_df.columns:
                report_df = report_df.sort_values(['è§¸ç™¼AIO', 'æ¯æœˆæœå°‹é‡'], ascending=[False, False])
            
            # ä¿å­˜ç‚º CSV
            report_df.to_csv(file_path, index=False, encoding='utf-8-sig')
            
            # å¦‚æœé…ç½®ç‚º Excel æ ¼å¼ï¼Œä¹Ÿç”Ÿæˆ Excel æ–‡ä»¶
            if self.output_format.lower() == 'excel':
                excel_filename = self._generate_filename('aio_analysis_report', 'xlsx')
                excel_path = self.output_dir / excel_filename
                
                with pd.ExcelWriter(excel_path, engine='openpyxl') as writer:
                    report_df.to_excel(writer, sheet_name='AIOåˆ†æå ±å‘Š', index=False)
                    
                    # æ·»åŠ æ‘˜è¦å·¥ä½œè¡¨
                    summary_df = pd.DataFrame([data['summary_stats']])
                    summary_df.to_excel(writer, sheet_name='åˆ†ææ‘˜è¦', index=False)
                
                self.logger.info(f"Excel å ±å‘Šå·²ç”Ÿæˆ: {excel_path}")
        
        else:
            # å¦‚æœæ²’æœ‰æ•¸æ“šï¼Œå‰µå»ºç©ºå ±å‘Š
            empty_df = pd.DataFrame(columns=['ç›®æ¨™é—œéµå­—', 'æ¯æœˆæœå°‹é‡', 'ç«¶çˆ­ç¨‹åº¦', 'è§¸ç™¼AIO'])
            empty_df.to_csv(file_path, index=False, encoding='utf-8-sig')
        
        self.logger.info(f"ä¸»å ±å‘Šå·²ç”Ÿæˆ: {file_path}")
        return file_path
    
    async def _generate_summary_report(self, data: Dict[str, Any]) -> Path:
        """ç”Ÿæˆæ‘˜è¦å ±å‘Š"""
        filename = self._generate_filename('aio_summary', 'json')
        file_path = self.output_dir / filename
        
        summary = {
            'report_metadata': {
                'generated_at': data['timestamp'].isoformat(),
                'report_type': 'AIO æ½›åŠ›åˆ†ææ‘˜è¦',
                'version': '1.0.0'
            },
            'analysis_summary': data['summary_stats'],
            'gsc_summary': data.get('gsc_stats', {}),
            'top_keywords': self._get_top_keywords(data['analysis_data']),
            'recommendations': self._generate_recommendations(data)
        }
        
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(summary, f, ensure_ascii=False, indent=2, default=str)
        
        self.logger.info(f"æ‘˜è¦å ±å‘Šå·²ç”Ÿæˆ: {file_path}")
        return file_path
    
    def _get_top_keywords(self, analysis_df: pd.DataFrame, top_n: int = 10) -> Dict[str, List]:
        """ç²å–é‡è¦é—œéµå­—åˆ—è¡¨"""
        if analysis_df.empty:
            return {}
        
        top_keywords = {}
        
        # è§¸ç™¼ AIO çš„é«˜æœå°‹é‡é—œéµå­—
        if 'triggers_aio' in analysis_df.columns and 'search_volume' in analysis_df.columns:
            aio_keywords = analysis_df[analysis_df['triggers_aio'] == True]
            if not aio_keywords.empty:
                top_aio = aio_keywords.nlargest(top_n, 'search_volume')
                top_keywords['high_volume_aio'] = top_aio[['keyword_idea', 'search_volume']].to_dict('records')
        
        # é«˜æœå°‹é‡ä½†æœªè§¸ç™¼ AIO çš„é—œéµå­—ï¼ˆæ©Ÿæœƒé—œéµå­—ï¼‰
        if 'triggers_aio' in analysis_df.columns and 'search_volume' in analysis_df.columns:
            non_aio_keywords = analysis_df[analysis_df['triggers_aio'] == False]
            if not non_aio_keywords.empty:
                top_opportunities = non_aio_keywords.nlargest(top_n, 'search_volume')
                top_keywords['opportunity_keywords'] = top_opportunities[['keyword_idea', 'search_volume']].to_dict('records')
        
        return top_keywords
    
    def _generate_recommendations(self, data: Dict[str, Any]) -> List[str]:
        """ç”Ÿæˆåˆ†æå»ºè­°"""
        recommendations = []
        stats = data['summary_stats']
        
        if not stats:
            return ["ç„¡è¶³å¤ æ•¸æ“šç”Ÿæˆå»ºè­°"]
        
        aio_percentage = stats.get('aio_percentage', 0)
        total_keywords = stats.get('total_keywords', 0)
        
        # åŸºæ–¼ AIO è§¸ç™¼ç‡çš„å»ºè­°
        if aio_percentage > 50:
            recommendations.append(f"âœ… AIO è§¸ç™¼ç‡è¼ƒé«˜ ({aio_percentage}%)ï¼Œè¡¨ç¤ºæ‚¨çš„é—œéµå­—ç­–ç•¥èˆ‡ AI Overview è¶¨å‹¢ä¸€è‡´")
        elif aio_percentage > 20:
            recommendations.append(f"âš ï¸ AIO è§¸ç™¼ç‡ä¸­ç­‰ ({aio_percentage}%)ï¼Œå»ºè­°å„ªåŒ–å…§å®¹ä»¥æé«˜ AI Overview æ›å…‰æ©Ÿæœƒ")
        else:
            recommendations.append(f"ğŸ”´ AIO è§¸ç™¼ç‡è¼ƒä½ ({aio_percentage}%)ï¼Œå»ºè­°é‡æ–°è©•ä¼°é—œéµå­—ç­–ç•¥ä¸¦é—œæ³¨å•ç­”å‹å…§å®¹")
        
        # åŸºæ–¼é—œéµå­—æ•¸é‡çš„å»ºè­°
        if total_keywords < 50:
            recommendations.append("ğŸ“ˆ å»ºè­°æ“´å±•æ›´å¤šç›¸é—œé—œéµå­—ä»¥ç²å¾—æ›´å…¨é¢çš„ AIO æ½›åŠ›åˆ†æ")
        
        # åŸºæ–¼æœå°‹é‡çš„å»ºè­°
        search_volume_stats = stats.get('search_volume_stats', {})
        if search_volume_stats:
            avg_volume = search_volume_stats.get('avg_volume', 0)
            if avg_volume > 1000:
                recommendations.append("ğŸ¯ å¹³å‡æœå°‹é‡è¼ƒé«˜ï¼Œé€™äº›é—œéµå­—å…·æœ‰è‰¯å¥½çš„æµé‡æ½›åŠ›")
            elif avg_volume < 100:
                recommendations.append("ğŸ’¡ å¹³å‡æœå°‹é‡è¼ƒä½ï¼Œè€ƒæ…®åŠ å…¥æ›´å¤šé«˜æœå°‹é‡çš„é•·å°¾é—œéµå­—")
        
        # ç«¶çˆ­ç¨‹åº¦å»ºè­°
        competition_dist = stats.get('competition_distribution', {})
        if competition_dist:
            high_competition = competition_dist.get('HIGH', 0)
            total_comp = sum(competition_dist.values())
            if high_competition / total_comp > 0.5:
                recommendations.append("âš”ï¸ é«˜ç«¶çˆ­é—œéµå­—è¼ƒå¤šï¼Œå»ºè­°å°‹æ‰¾ç«¶çˆ­ç¨‹åº¦è¼ƒä½çš„æ›¿ä»£é—œéµå­—")
        
        return recommendations
    
    async def _generate_statistics_report(self, data: Dict[str, Any]) -> Path:
        """ç”Ÿæˆçµ±è¨ˆåˆ†æå ±å‘Š"""
        filename = self._generate_filename('aio_statistics', 'txt')
        file_path = self.output_dir / filename
        
        stats = data['summary_stats']
        gsc_stats = data.get('gsc_stats', {})
        
        report_content = f"""
AIO æ½›åŠ›åˆ†æçµ±è¨ˆå ±å‘Š
==================
ç”Ÿæˆæ™‚é–“: {data['timestamp'].strftime('%Y-%m-%d %H:%M:%S')}

é—œéµå­—åˆ†æçµ±è¨ˆ
--------------
ç¸½é—œéµå­—æ•¸é‡: {stats.get('total_keywords', 0):,}
è§¸ç™¼ AIO é—œéµå­—: {stats.get('aio_keywords', 0):,}
AIO è§¸ç™¼ç‡: {stats.get('aio_percentage', 0):.2f}%

æœå°‹é‡çµ±è¨ˆ
----------
ç¸½æœå°‹é‡: {stats.get('search_volume_stats', {}).get('total_volume', 0):,}
å¹³å‡æœå°‹é‡: {stats.get('search_volume_stats', {}).get('avg_volume', 0):,.0f}
ä¸­ä½æ•¸æœå°‹é‡: {stats.get('search_volume_stats', {}).get('median_volume', 0):,.0f}
æœ€é«˜æœå°‹é‡: {stats.get('search_volume_stats', {}).get('max_volume', 0):,}

ç«¶çˆ­ç¨‹åº¦åˆ†å¸ƒ
------------
"""
        
        competition_dist = stats.get('competition_distribution', {})
        for level, count in competition_dist.items():
            percentage = (count / stats.get('total_keywords', 1)) * 100
            report_content += f"{level}: {count:,} ({percentage:.1f}%)\n"
        
        if gsc_stats:
            report_content += f"""
GSC æ•¸æ“šçµ±è¨ˆ
------------
ç¸½æŸ¥è©¢æ•¸: {gsc_stats.get('total_queries', 0):,}
ç¸½é»æ“Šæ•¸: {gsc_stats.get('total_clicks', 0):,}
ç¸½æ›å…‰æ•¸: {gsc_stats.get('total_impressions', 0):,}
å¹³å‡ CTR: {gsc_stats.get('avg_ctr', 0):.3f}
å¹³å‡æ’å: {gsc_stats.get('avg_position', 0):.1f}
"""
        
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(report_content)
        
        self.logger.info(f"çµ±è¨ˆå ±å‘Šå·²ç”Ÿæˆ: {file_path}")
        return file_path
    
    async def _generate_charts(self, data: Dict[str, Any]):
        """ç”Ÿæˆåœ–è¡¨ï¼ˆéœ€è¦ matplotlibï¼‰"""
        try:
            import matplotlib.pyplot as plt
            import matplotlib
            matplotlib.use('Agg')  # ä½¿ç”¨éäº’å‹•å¼å¾Œç«¯
            
            plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'SimHei']
            plt.rcParams['axes.unicode_minus'] = False
            
            analysis_df = data['analysis_data']
            if analysis_df.empty:
                return
            
            # AIO è§¸ç™¼ç‡é¤…åœ–
            if 'triggers_aio' in analysis_df.columns:
                await self._create_aio_pie_chart(analysis_df)
            
            # æœå°‹é‡åˆ†å¸ƒç›´æ–¹åœ–
            if 'search_volume' in analysis_df.columns:
                await self._create_search_volume_histogram(analysis_df)
            
            # ç«¶çˆ­ç¨‹åº¦åˆ†å¸ƒæŸ±ç‹€åœ–
            if 'competition_level' in analysis_df.columns:
                await self._create_competition_bar_chart(analysis_df)
            
        except ImportError:
            self.logger.warning("matplotlib æœªå®‰è£ï¼Œè·³éåœ–è¡¨ç”Ÿæˆ")
        except Exception as e:
            self.logger.error(f"ç”Ÿæˆåœ–è¡¨æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
    
    async def _create_aio_pie_chart(self, df: pd.DataFrame):
        """å‰µå»º AIO è§¸ç™¼ç‡é¤…åœ–"""
        import matplotlib.pyplot as plt
        
        aio_counts = df['triggers_aio'].value_counts()
        labels = ['è§¸ç™¼ AIO', 'æœªè§¸ç™¼ AIO']
        
        plt.figure(figsize=(8, 6))
        plt.pie(aio_counts.values, labels=labels, autopct='%1.1f%%', startangle=90)
        plt.title('AIO è§¸ç™¼ç‡åˆ†å¸ƒ')
        
        filename = self._generate_filename('aio_distribution', 'png')
        plt.savefig(self.output_dir / filename, dpi=300, bbox_inches='tight')
        plt.close()
        
        self.logger.info(f"AIO åˆ†å¸ƒåœ–å·²ç”Ÿæˆ: {filename}")
    
    async def _create_search_volume_histogram(self, df: pd.DataFrame):
        """å‰µå»ºæœå°‹é‡åˆ†å¸ƒç›´æ–¹åœ–"""
        import matplotlib.pyplot as plt
        
        plt.figure(figsize=(10, 6))
        plt.hist(df['search_volume'], bins=20, edgecolor='black', alpha=0.7)
        plt.xlabel('æœå°‹é‡')
        plt.ylabel('é—œéµå­—æ•¸é‡')
        plt.title('æœå°‹é‡åˆ†å¸ƒ')
        plt.grid(True, alpha=0.3)
        
        filename = self._generate_filename('search_volume_distribution', 'png')
        plt.savefig(self.output_dir / filename, dpi=300, bbox_inches='tight')
        plt.close()
        
        self.logger.info(f"æœå°‹é‡åˆ†å¸ƒåœ–å·²ç”Ÿæˆ: {filename}")
    
    async def _create_competition_bar_chart(self, df: pd.DataFrame):
        """å‰µå»ºç«¶çˆ­ç¨‹åº¦åˆ†å¸ƒæŸ±ç‹€åœ–"""
        import matplotlib.pyplot as plt
        
        competition_counts = df['competition_level'].value_counts()
        
        plt.figure(figsize=(10, 6))
        bars = plt.bar(competition_counts.index, competition_counts.values)
        plt.xlabel('ç«¶çˆ­ç¨‹åº¦')
        plt.ylabel('é—œéµå­—æ•¸é‡')
        plt.title('ç«¶çˆ­ç¨‹åº¦åˆ†å¸ƒ')
        
        # åœ¨æŸ±ç‹€åœ–ä¸Šé¡¯ç¤ºæ•¸å€¼
        for bar in bars:
            height = bar.get_height()
            plt.text(bar.get_x() + bar.get_width()/2., height,
                    f'{int(height)}', ha='center', va='bottom')
        
        filename = self._generate_filename('competition_distribution', 'png')
        plt.savefig(self.output_dir / filename, dpi=300, bbox_inches='tight')
        plt.close()
        
        self.logger.info(f"ç«¶çˆ­ç¨‹åº¦åˆ†å¸ƒåœ–å·²ç”Ÿæˆ: {filename}")
    
    async def _generate_metadata(self, data: Dict[str, Any], main_report_path: Path):
        """ç”Ÿæˆå ±å‘Šå…ƒæ•¸æ“š"""
        metadata = {
            'report_info': {
                'generated_at': data['timestamp'].isoformat(),
                'main_report': str(main_report_path),
                'output_directory': str(self.output_dir),
                'format': self.output_format
            },
            'data_summary': data['summary_stats'],
            'file_list': [f.name for f in self.output_dir.iterdir() if f.is_file()]
        }
        
        filename = self._generate_filename('report_metadata', 'json')
        metadata_path = self.output_dir / filename
        
        with open(metadata_path, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2, default=str)
        
        self.logger.info(f"å ±å‘Šå…ƒæ•¸æ“šå·²ç”Ÿæˆ: {metadata_path}")
    
    def cleanup_old_reports(self, keep_days: int = 30):
        """æ¸…ç†èˆŠå ±å‘Šæ–‡ä»¶"""
        if not self.output_dir.exists():
            return
        
        cutoff_time = datetime.now().timestamp() - (keep_days * 24 * 3600)
        
        removed_count = 0
        for file_path in self.output_dir.iterdir():
            if file_path.is_file() and file_path.stat().st_mtime < cutoff_time:
                try:
                    file_path.unlink()
                    removed_count += 1
                except Exception as e:
                    self.logger.warning(f"ç„¡æ³•åˆªé™¤èˆŠæ–‡ä»¶ {file_path}: {e}")
        
        if removed_count > 0:
            self.logger.info(f"å·²æ¸…ç† {removed_count} å€‹è¶…é {keep_days} å¤©çš„èˆŠå ±å‘Šæ–‡ä»¶")
